{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# this can be used to switch on/off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple TF2 + Keras model for segmentation (to detect cell nuclei)\n",
    "This notebook the entire workflow of training an ANN with [TensorFlow 2](https://www.tensorflow.org/) using the keras API and exporting the trained model to the [CZModel format](https://github.com/zeiss-microscopy/OAD/blob/master/Machine_Learning/docs/ann_model_specification.md) to be ready for use within the [Intellesis](https://www.zeiss.de/mikroskopie/produkte/mikroskopsoftware/zen-intellesis-image-segmentation-by-deep-learning.html) infrastructure.\n",
    "\n",
    "* The trained model is rather simple (for demo purposed) and trained on a small test dataset.\n",
    "* **Therefore, this notebook is meant to be understood as a guide for exporting trained models**\n",
    "* **The notebook does not show how train a model correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# required imports to train a simple TF2 + Keras model for segmentation and package it as CZMODEL\n",
    "# the CZMODEL can be then imported in ZEN and used for segmentation and image analysis workflows\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# those functions are provided by the PyPi package called czmodel (by ZEISS)\n",
    "from czmodel.util.preprocessing import PerImageStandardization\n",
    "from czmodel.model_metadata import ModelMetadata, ModelSpec\n",
    "from czmodel import convert_from_model_spec, convert_from_json_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# optional suppress TF warnings\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Pipeline\n",
    "This section describes a simple training procedure that creates a trained Keras model.\n",
    "\n",
    "* Therefore, it only represents the custom training procedure\n",
    "* Such procedure will vary from case to case and will contain more sophisticated ways to generate an optimized Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Define parameters for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the parameters for loading the training data\n",
    "\n",
    "# place the original *.png images here\n",
    "IMAGES_FOLDER = 'data/nuclei_images/'\n",
    "\n",
    "# place the respective label *.png images here\n",
    "# masks images have one channel (0=background and 1=nucleus)\n",
    "MASKS_FOLDER = 'data/nuclei_masks/'\n",
    "\n",
    "# define the number of channels\n",
    "# this means using a grayscale image with one channel only\n",
    "CHANNELS = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Read images\n",
    "This part contains the logic to read pairs of images and label masks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Read the images\n",
    "# This part contains the logic to read pairs of images and label masks for training !\n",
    "\n",
    "# the the sample images\n",
    "sample_images = sorted([os.path.join(IMAGES_FOLDER, f) for f in os.listdir(IMAGES_FOLDER) \n",
    "                        if os.path.isfile(os.path.join(IMAGES_FOLDER, f))])\n",
    "\n",
    "# get the maks\n",
    "sample_masks = sorted([os.path.join(MASKS_FOLDER, f) for f in os.listdir(MASKS_FOLDER) \n",
    "                       if os.path.isfile(os.path.join(MASKS_FOLDER, f))])\n",
    "\n",
    "# load images as numpy arrays\n",
    "images_loaded = np.asarray([tf.image.decode_image(tf.io.read_file(sample_path), channels=CHANNELS).numpy() \n",
    "                            for sample_path in sample_images])\n",
    "\n",
    "# load labels as numpy arrays\n",
    "masks_loaded = np.asarray([tf.one_hot(tf.image.decode_image(tf.io.read_file(sample_path), channels=1)[...,0], depth=2).numpy()\n",
    "                           for sample_path in sample_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remark: For details see [tf.one_hot](https://www.tensorflow.org/api_docs/python/tf/one_hot)\n",
    "\n",
    "`tf.one_hot creates X channels from X labels: 1 => [0.0, 1.0], 0 => [1.0, 0.0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a simple model\n",
    "This part defines a simple Keras model with two convolutional layers and softmax activation at the output node. It is also possible to add pre.processing layers to the model here.\n",
    "\n",
    "In order to make the model robust to input scaling we standardize each image before training with the PerImageStandardization layer provided by the `czmodel` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define simple Keras model with two convolutional layers and softmax activation at the output node\n",
    "\n",
    "model = tf.keras.models.Sequential([PerImageStandardization(input_shape=(128, 128, 1)),\n",
    "                                    tf.keras.layers.Conv2D(16, 3, padding='same'), \n",
    "                                    tf.keras.layers.Conv2D(2, 1, activation='softmax', padding='same')])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fit the model to the loaded data\n",
    "This part fits the model to the loaded data and evaluates it on the training data. In this test example we do not care about an actual evaluation of the model using validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 6ms/sample - loss: 0.6216 - categorical_accuracy: 0.8455\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5985 - categorical_accuracy: 0.8503\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5765 - categorical_accuracy: 0.8548\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5532 - categorical_accuracy: 0.8597\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5326 - categorical_accuracy: 0.8666\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5095 - categorical_accuracy: 0.8697\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.4869 - categorical_accuracy: 0.8715\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.4643 - categorical_accuracy: 0.8755\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.4434 - categorical_accuracy: 0.8805\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.4211 - categorical_accuracy: 0.8829\n",
      "200/200 [==============================] - 0s 2ms/sample - loss: 0.4081 - categorical_accuracy: 0.8820\n",
      "The model achieves 88.20095658302307% accuracy on the training data.\n"
     ]
    }
   ],
   "source": [
    "# define number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(images_loaded, masks_loaded,\n",
    "          batch_size=32,\n",
    "          epochs=num_epochs)\n",
    "\n",
    "# get the loss and acuary values\n",
    "loss, accuracy = model.evaluate(images_loaded, masks_loaded)\n",
    "\n",
    "# show the final accuracy achieved\n",
    "print(\"The model achieves {}% accuracy on the training data.\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a CZModel from the trained Keras model\n",
    "In this section we export the trained model to the CZModel format using the czmodel library and some additional meta data all possible parameter choices are described in the [ANN model specification](https://github.com/zeiss-microscopy/OAD/blob/master/Machine_Learning/docs/ann_model_specification.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define Meta-Data\n",
    "We first define the meta-data needed to run the model within the Intellesis infrastructure. The `czmodel` package offers a named tuple `ModelMetadata` that allows to either parse as JSON file as described in the [specification document](https://github.com/zeiss-microscopy/OAD/blob/master/Machine_Learning/docs/ann_model_specification.md) or to directly specify the parameters as shown below.\n",
    "\n",
    "### Create a Model Specification Object\n",
    "The export functions provided by the `czmodel` package expect a `ModelSpec` tuple that features the Keras model to be exported and the corresponding model meda-data.\n",
    "\n",
    "Therefore, we wrap our model and the `model_metadata` instane into a `ModelSpec` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define the model metadata\n",
    "model_metadata = ModelMetadata.from_params(name='Simple_Nuclei_SegmentationModel', \n",
    "                                           color_handling='ConvertToMonochrome',\n",
    "                                           pixel_type='Gray16',\n",
    "                                           classes=[\"Background\", \"Nucleus\"],\n",
    "                                           border_size=8)\n",
    "\n",
    "\n",
    "# Create a model specification object used for conversion\n",
    "model_spec = ModelSpec(model=model, model_metadata=model_metadata)\n",
    "\n",
    "# Define dimensions - ZEN Intellesis requires fully defined spatial dimensions.\n",
    "# This is the tile size used by the ZEN TilingClient to pass an image to the segmentation service.\n",
    "\n",
    "# Important: The tile size has to sufficiently small to be handled by your hardware!\n",
    "# Optional: Define target spatial dimensions of the model for inference.\n",
    "spatial_dims = (1024, 1024)  # Optional: Target spatial dimensions of the model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perform model export into *.czmodel file format\n",
    "\n",
    "The `czmodel` library offers two functions to perform the actual export. \n",
    "\n",
    "* `convert_from_json_spec` allows to provide a JSON file with all information to convert a model in SavedModel format on disk to a `.czmodel` file that can be loaded with ZEN.\n",
    "* `convert_from_model_spec` expects a `ModelSpec` object, an output path and name and optionally target spatial dimensions for the expected input of the exported model. From this information it creates a `.czmodel` file containing the specified model.\n",
    "\n",
    "```python\n",
    "convert_from_model_spec(model_spec=model_spec, \n",
    "                        output_path=folder_to_store_czmodel, \n",
    "                        output_name=name_of_the_model, \n",
    "                        spatial_dims=spatial_dims)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# perform the actual model export directly into a *.czmodel file\n",
    "\n",
    "convert_from_model_spec(model_spec=model_spec, \n",
    "                        output_path='./czmodel_output', \n",
    "                        output_name='simple_nuclei_segmodel', \n",
    "                        spatial_dims=spatial_dims)\n",
    "\n",
    "# In the example above there will be a \"\"./czmodel_output/simple_nuclei_segmodel.czmodel\" file saved on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remarks\n",
    "The generated .czmodel file can be directly loaded into ZEN Intellesis to perform segmentation tasks with the trained model.\n",
    "If there is already a trained model in SavedModel format present on disk, it can also be converted by providing a meta-data JSON file as described in the [specification](https://github.com/zeiss-microscopy/OAD/blob/master/Machine_Learning/docs/ann_model_specification.md).\n",
    "\n",
    "The following JSON document describes the same meta-data applied in the use case above:\n",
    "\n",
    "```json\n",
    "{\n",
    "\"Name\": \"SimpleNucleiModel From JSON\",\n",
    "\"BorderSize\": 8,\n",
    "\"ColorHandling\": \"ConvertToMonochrome\",\n",
    "\"PixelType\": \"Gray16\",\n",
    "\"Classes\": [\"Background\", \"Nuclei\"],\n",
    "\"ModelPath\": \"saved_tf2_model_output\",\n",
    "}\n",
    "```\n",
    "\n",
    "This information can be copied to a file e.g. in the current working directory `./model_spec.json` that also contains the trained model in SavedModel format e.g. generated by the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# save the trained TF2.SavedModel as a folder structure\n",
    "# The folder + the JSON file can be also used to import the model in ZEN (still fas a bug)\n",
    "\n",
    "model.save('./saved_tf2_model_output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The CZMODEL file (which is essentially a zip file) contains:\n",
    "\n",
    "* model **guid** file: modelid=e47aabbd-8269-439c-b142-78feec2ed2dd\n",
    "* model file: modelid=e47aabbd-8269-439c-b142-78feec2ed2dd.model\n",
    "* model description: e47aabbd-8269-439c-b142-78feec2ed2dd.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example of a model XML description**\n",
    "\n",
    "<img src=\"./mdpics/model_xml.png\" width =\"1200\" height=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To import the newly created model just use the **`Import`** function of the Intellesis Trainable Segmentation module in ZEN.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model1.png\" width =\"1200\" height=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Select the **`simple_nuclei_segmodel.czmodel`** file and press the **`Open`** button.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model2.png\" width =\"1200\" height=1200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the IP-function **`Segmentation`** to segment an image using the imported CZMODEL (containing the trained network).\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IPseg.png\" width =\"1200\" height=1200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To use the trained model to analyse an image there are two main options\n",
    "\n",
    "1. directly create an Image Analysis Setting based on the model (no class hierachy, but very simple)\n",
    "2. assign the trained model to s specfic class inside a customized image analyssis setting (shown below)\n",
    "\n",
    "The crucial step (when not using option 1) is the Select the correct **`Class Segmentation Method`** inside the Image Analysis Wizard.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IA1.png\" width =\"1000\" height=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the **`Select Model`** function to assign the trained model and the actual **class** (from the trained model) of interes to assign the model / class to the respective object inside the image analysis setting.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IA2.png\" width =\"1000\" height=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now the trained model will be used to segment the image. The built-in ZEN Tiling Client automatically  to chucnk the image and deal with cmplex dimensions, like Use the **`Scenes`** etc.\n",
    "\n",
    "Additional Porst-Processing option, incl. a Minimum Confidence Threshold can be applied to further refine the results.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IA3.png\" width =\"1000\" height=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, the model can be loaded into ZEN by using the **Import** function on the **JSON file**. \n",
    "\n",
    "If the model is supposed to be provided to other parties it is usually easier to exchange .czmodel files instead of SavedModel directories with corresponding JSON meta-data files.\n",
    "\n",
    "The `czmodel` library also provides a `convert_from_json_spec` function that accepts the above mentioned JSON file and creates a CZModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This ia additional way how to create a CZMODEL from a saved TF2 model on disk + JSON file.\n",
    "# The currently recommended way to to create the CZMODEL directly by using czmodel.convert_from_model_spec\n",
    "# the path to the TF2.SavedModel folder is defined in the JSON shown above\n",
    "\n",
    "convert_from_json_spec(model_spec_path='model_spec.json',\n",
    "                       output_path='model_from_json',\n",
    "                       output_name = 'simple_nuclei_segmodel_from_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* the path to the saved model folder is defined in the JSON shown above\n",
    "\n",
    "* **Remark: Due a TF 2.1 bug reloading a model and defining the new spatial dimension does currently not work correctly.**\n",
    "\n",
    "```python\n",
    "convert_from_json_spec(model_spec_path='model_spec.json',\n",
    "                       output_path='model_from_json',\n",
    "                       output_name = 'simple_nuclei_segmodel_from_json',\n",
    "                       spatial_dims = (1024, 1024)\n",
    "                      )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Use the commands below from a terminal to present the notebook as a slideshow.\n",
    "\n",
    "`\n",
    "jupyter nbconvert train_simple_TF2_segmentation_model.ipynb --to slides --post serve \n",
    "    --SlidesExporter.reveal_theme=serif \n",
    "    --SlidesExporter.reveal_scroll=True \n",
    "    --SlidesExporter.reveal_transition=none\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
